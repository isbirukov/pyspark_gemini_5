{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19bf906-f09d-4ab1-9007-b853f502e8b6",
   "metadata": {},
   "source": [
    "#### Шаг 1: Генерация \"грязного\" CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44a38fe-d5df-4854-ac56-85ddddbf93f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV файлы созданы. Проверьте папку data/raw_sales через терминал.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IO_Practice\").getOrCreate()\n",
    "\n",
    "# Создаем папку для данных\n",
    "os.makedirs(\"/home/jovyan/workspace/data/raw_sales\", exist_ok=True)\n",
    "\n",
    "# Генерируем данные\n",
    "data = [\n",
    "    (\"2023-01-01\", \"iPhone\", \"US\", 1000),\n",
    "    (\"2023-01-01\", \"Samsung\", \"KR\", 900),\n",
    "    (\"2023-01-02\", \"Xiaomi\", \"CN\", 400),\n",
    "    (\"2023-01-02\", \"iPhone\", \"US\", 1000)\n",
    "]\n",
    "columns = [\"date\", \"product\", \"country\", \"amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Пишем в CSV (эмулируем входящие данные)\n",
    "# header=True - записать заголовки\n",
    "df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"/home/jovyan/workspace/data/raw_sales\")\n",
    "\n",
    "print(\"CSV файлы созданы. Проверьте папку data/raw_sales через терминал.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48242ec0-c186-43a7-9855-9192b79976b0",
   "metadata": {},
   "source": [
    "#### Шаг 2: Правильное чтение CSV\n",
    "Теперь представим, что мы начали ETL-процесс. Читаем CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b728580-14ee-40e3-8a4e-da41da9458c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+\n",
      "|      date|product|country|amount|\n",
      "+----------+-------+-------+------+\n",
      "|2023-01-01| iPhone|     US|  1000|\n",
      "|2023-01-01|Samsung|     KR|   900|\n",
      "|2023-01-02| iPhone|     US|  1000|\n",
      "|2023-01-02| Xiaomi|     CN|   400|\n",
      "+----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 1. Определяем схему (Best Practice)\n",
    "sales_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 2. Читаем\n",
    "raw_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(sales_schema) \\\n",
    "    .load(\"/home/jovyan/workspace/data/raw_sales\")\n",
    "\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983e248-0077-4298-b806-a1eb7f20a0e5",
   "metadata": {},
   "source": [
    "#### Шаг 3: Конвертация в Parquet с Партиционированием\n",
    "Мы хотим сохранить данные так, чтобы потом быстро искать продажи по странам. Используем partitionBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9506f9e-bae1-4f94-b48f-092c17a82b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные записаны в Parquet с партиционированием.\n"
     ]
    }
   ],
   "source": [
    "# Пишем в Parquet, разделяя по стране\n",
    "raw_df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\") \\\n",
    "    .save(\"/home/jovyan/workspace/data/processed_sales\")\n",
    "\n",
    "print(\"Данные записаны в Parquet с партиционированием.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68dc2b5-49cb-4226-9e8b-ae2dafeb3f69",
   "metadata": {},
   "source": [
    "#### Шаг 4: Анализ структуры на диске (Терминал)\n",
    "Выполните это прямо в ноутбуке через !:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c0717b-463a-4964-b34d-94ffad43bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x 5 jovyan users 4096 Dec 16 12:24  .\n",
      "drwxr-xr-x 4 jovyan users 4096 Dec 16 12:24  ..\n",
      "drwxr-xr-x 2 jovyan users 4096 Dec 16 12:24 'country=CN'\n",
      "drwxr-xr-x 2 jovyan users 4096 Dec 16 12:24 'country=KR'\n",
      "drwxr-xr-x 2 jovyan users 4096 Dec 16 12:24 'country=US'\n",
      "-rw-r--r-- 1 jovyan users    0 Dec 16 12:24  _SUCCESS\n",
      "-rw-r--r-- 1 jovyan users    8 Dec 16 12:24  ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -la /home/jovyan/workspace/data/processed_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6a7ba-fc5b-4683-a352-76da4bdaa015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

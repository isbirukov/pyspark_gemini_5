{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048428f2-4bb2-4e28-b2d4-18e0442c5591",
   "metadata": {},
   "source": [
    "#### Шаг 1: Генерация данных (E-commerce logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c09941-2a47-4506-a8b8-94c443f35848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+---+\n",
      "|      date|product|country| price|qty|\n",
      "+----------+-------+-------+------+---+\n",
      "|2023-01-01| iPhone|     US|1200.0|  1|\n",
      "|2023-01-01|Samsung|     KR| 900.0|  2|\n",
      "|2023-01-01| Xiaomi|     CN| 400.0|  5|\n",
      "|2023-01-02| iPhone|     US|1200.0|  1|\n",
      "|2023-01-02|Samsung|     US| 900.0|  1|\n",
      "|2023-01-03|  Nokia|     FI|  50.0| 10|\n",
      "|2023-01-03| Xiaomi|     CN| 400.0|  0|\n",
      "+----------+-------+-------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Transformations_Lab\").getOrCreate()\n",
    "\n",
    "# Генерируем данные о покупках\n",
    "data = [\n",
    "    (\"2023-01-01\", \"iPhone\", \"US\", 1200.0, 1),\n",
    "    (\"2023-01-01\", \"Samsung\", \"KR\", 900.0, 2),\n",
    "    (\"2023-01-01\", \"Xiaomi\", \"CN\", 400.0, 5),\n",
    "    (\"2023-01-02\", \"iPhone\", \"US\", 1200.0, 1),\n",
    "    (\"2023-01-02\", \"Samsung\", \"US\", 900.0, 1), # Samsung продан в US\n",
    "    (\"2023-01-03\", \"Nokia\", \"FI\", 50.0, 10),\n",
    "    (\"2023-01-03\", \"Xiaomi\", \"CN\", 400.0, 0)  # Возврат (qty=0) или ошибка\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"qty\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fecd427-b104-4632-b19a-3bb36e5b10a7",
   "metadata": {},
   "source": [
    "#### Шаг 2: Узкие трансформации (Очистка и Обогащение)\n",
    "Задача: Оставить только дорогие продажи (price > 100) и посчитать общую сумму (total = price * qty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f5354f-68ee-4280-86eb-a6537380083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+---------+\n",
      "|      date|country|product|total_amt|\n",
      "+----------+-------+-------+---------+\n",
      "|2023-01-01|     US| iPhone|   1200.0|\n",
      "|2023-01-01|     KR|Samsung|   1800.0|\n",
      "|2023-01-01|     CN| Xiaomi|   2000.0|\n",
      "|2023-01-02|     US| iPhone|   1200.0|\n",
      "|2023-01-02|     US|Samsung|    900.0|\n",
      "+----------+-------+-------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [date#0, country#2, product#1, (price#3 * cast(qty#4 as double)) AS total_amt#31]\n",
      "+- *(1) Filter ((isnotnull(qty#4) AND isnotnull(price#3)) AND ((qty#4 > 0) AND (price#3 > 100.0)))\n",
      "   +- *(1) Scan ExistingRDD[date#0,product#1,country#2,price#3,qty#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Цепочка (Chaining) - Best Practice\n",
    "# Читается сверху вниз. Spark оптимизирует это в один проход.\n",
    "\n",
    "df_clean = df \\\n",
    "    .filter( (F.col(\"qty\") > 0) & (F.col(\"price\") > 100) ) \\\n",
    "    .withColumn(\"total_amt\", F.col(\"price\") * F.col(\"qty\")) \\\n",
    "    .select(\"date\", \"country\", \"product\", \"total_amt\")\n",
    "\n",
    "# .filter(...) - Narrow (фильтруем строки на месте)\n",
    "# .withColumn(...) - Narrow (считаем математику на месте)\n",
    "# .select(...) - Narrow (выбираем колонки на месте)\n",
    "\n",
    "df_clean.show()\n",
    "\n",
    "# Посмотрим план выполнения (Physical Plan)\n",
    "df_clean.explain()\n",
    "# Ищите ключевые слова: Filter, Project (это Select). Никакого \"Exchange\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a4818-ab32-45d3-a280-24b5d6f98382",
   "metadata": {},
   "source": [
    "#### Шаг 3: Широкие трансформации (Агрегация)\n",
    "Задача: Посчитать выручку по странам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf11d7b-f124-4f83-8884-034b9eb6aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+---------+\n",
      "|country|total_revenue|sales_count|max_check|\n",
      "+-------+-------------+-----------+---------+\n",
      "|     US|       3300.0|          3|   1200.0|\n",
      "|     CN|       2000.0|          1|   2000.0|\n",
      "|     KR|       1800.0|          1|   1800.0|\n",
      "+-------+-------------+-----------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_revenue#64 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_revenue#64 DESC NULLS LAST, 8), ENSURE_REQUIREMENTS, [plan_id=125]\n",
      "      +- HashAggregate(keys=[country#2], functions=[sum(total_amt#31), count(product#1), max(total_amt#31)])\n",
      "         +- Exchange hashpartitioning(country#2, 8), ENSURE_REQUIREMENTS, [plan_id=122]\n",
      "            +- HashAggregate(keys=[country#2], functions=[partial_sum(total_amt#31), partial_count(product#1), partial_max(total_amt#31)])\n",
      "               +- Project [country#2, product#1, (price#3 * cast(qty#4 as double)) AS total_amt#31]\n",
      "                  +- Filter ((isnotnull(qty#4) AND isnotnull(price#3)) AND ((qty#4 > 0) AND (price#3 > 100.0)))\n",
      "                     +- Scan ExistingRDD[date#0,product#1,country#2,price#3,qty#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy вызывает Shuffle\n",
    "df_stats = df_clean \\\n",
    "    .groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_amt\").alias(\"total_revenue\"),\n",
    "        F.count(\"product\").alias(\"sales_count\"),\n",
    "        F.max(\"total_amt\").alias(\"max_check\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"total_revenue\").desc()) # Сортировка - тоже Shuffle!\n",
    "\n",
    "df_stats.show()\n",
    "\n",
    "# Посмотрим план\n",
    "df_stats.explain()\n",
    "# Вы увидите: \"Exchange hashpartitioning\" - это Shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d7c3c8-0461-447a-9ab1-778231a03ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd2c49-6be4-45c4-8010-f7504d15b7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

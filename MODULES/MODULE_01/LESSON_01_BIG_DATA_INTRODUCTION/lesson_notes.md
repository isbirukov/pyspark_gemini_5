# Конспект Урока 1: Введение в Big Data и Spark

## 1. Big Data (3V)
* **Volume:** Огромный объем данных (ТБ, ПБ).
* **Velocity:** Высокая скорость генерации и обработки.
* **Variety:** Разнообразие форматов (структурированные, неструктурированные).

## 2. Масштабирование
* **Scale-Up (Вертикальное):** Один мощный сервер (дорого, ограниченно).
* **Scale-Out (Горизонтальное):** Много обычных компьютеров (дешево, масштабируемо) — **подход Spark**.

## 3. Apache Spark и PySpark
* **Apache Spark:** Движок для **распределенной обработки** Big Data.
* **PySpark:** Python API для Spark. Позволяет писать код на Python, делегируя распределенные задачи ядру Spark.

## 4. Ключевые Компоненты Архитектуры (Упрощенно)
* **Driver Program (Драйвер):** Выполняет код PySpark, планирует работу, создает `SparkSession`.
* **Cluster Manager:** Управляет ресурсами, выделяет место для **Executors**.
* **Executors (Исполнители):** Процессы на узлах кластера, выполняющие фактическую обработку данных.

## 5. Инициализация Сессии
* **Точка входа:** `SparkSession`
* **Код:**
    ```python
    from pyspark.sql import SparkSession
    spark = (
        SparkSession.builder
        .appName("MyAppName") # Best Practice: Осмысленное имя
        .getOrCreate()
    )
    # Завершение работы: spark.stop()
    ```

## 6. Анти-Паттерны
* ❌ Использование Spark для малых данных.
* ❌ Забывать `spark.stop()`.

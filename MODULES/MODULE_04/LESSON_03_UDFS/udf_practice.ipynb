{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3d4b6a-d1b0-440f-b093-c93cef2c9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UDF_Lab\").getOrCreate()\n",
    "\n",
    "# Генерируем 1 млн точек данных\n",
    "df = spark.range(0, 1000000).select(\n",
    "    (F.rand() * 100).alias(\"temperature\")  # температрура от 0 до 100\n",
    ")\n",
    "\n",
    "print(f\"Rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93444c6-514a-45f0-a712-76c6664a5ba9",
   "metadata": {},
   "source": [
    "### Standard Python UDF (Медленный способ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b161c6dc-91f8-4a9d-b21d-bd8d2ea3c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard UDF Time: 1.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. Пишем чистую функцию Python\n",
    "def categorize_temp(temp):\n",
    "    if temp < 10:\n",
    "        return \"Cold\"\n",
    "    elif temp < 25:\n",
    "        return \"Comfort\"\n",
    "    else:\n",
    "        return \"Hot\"\n",
    "\n",
    "# 2. Регистрируем как UDF (обязательно указываем тип возврата!)\n",
    "categorize_udf = F.udf(categorize_temp, StringType())\n",
    "\n",
    "# Применяем\n",
    "start = time.time()\n",
    "df.withColumn(\n",
    "    \"category\",\n",
    "    categorize_udf(F.col(\"temperature\"))).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "\n",
    "print(f\"Standard UDF Time: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea33ad-fd28-4864-a61b-a7e5e5352228",
   "metadata": {},
   "source": [
    "### Pandas UDF (Быстрый способ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2137b7a4-5b6f-4029-ae0d-309640e17ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas UDF Time: 1.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Декоратор региструет функцию. Тип указываем сразу.\n",
    "# В новейших версиях Spark (3.0+) синтаксис hint типов Python предпочтителен, но строковый тип тоже работает.\n",
    "@pandas_udf(StringType())\n",
    "def categorize_pandas_udf(temp_series: pd.Series) -> pd.Series:\n",
    "    # Мы работаем с вектором (Series), а не с числом!\n",
    "    return temp_series.apply(\n",
    "        lambda x: \"Cold\" if x < 10 else \"Comfort\" if x < 25 else \"Hot\"\n",
    "    )\n",
    "\n",
    "start = time.time()\n",
    "df.withColumn(\"category\", categorize_pandas_udf(F.col(\"temperature\"))).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(f\"Pandas UDF Time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Ожидание: Pandas UDF должен быть быстрее (иногда в разы, на сложных вычислениях).\n",
    "# На простых if-else разница может быть небольшой из-за накладных расходов Arrow, но на математике разрыв колоссальный."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61e9fb-7458-4511-85bc-4e5230491fee",
   "metadata": {},
   "source": [
    "### Нативный способ (Best Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b065faa-b195-4b84-9492-2a5de126fcad",
   "metadata": {},
   "source": [
    "Самый быстрый UDF — это отсутствие UDF. Всегда проверяйте, можно ли сделать это через F.when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d283fab-cf7a-4f5c-a919-62aa1cecdbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native Spark Time: 0.25 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df.withColumn(\"category\", \n",
    "    F.when(F.col(\"temperature\") < 10, \"Cold\")\n",
    "     .when(F.col(\"temperature\") < 25, \"Comfort\")\n",
    "     .otherwise(\"Hot\")\n",
    ").write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(f\"Native Spark Time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Это будет быстрее всех."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01346738-16f6-4dc6-98d0-a3c8a79e1446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

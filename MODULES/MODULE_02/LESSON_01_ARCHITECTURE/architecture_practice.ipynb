{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0955b8-0fdf-4900-b983-94853c228d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# 1. Создаем сессию с явным указанием ресурсов для наглядности\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Module2_Architecture_Lab\")\n",
    "         .master(\"local[*]\") # Используем все ядра локально\n",
    "         .config(\"spark.executor.memory\", \"1g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Ссылка на UI\n",
    "print(f\"Go to Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f5d695-ac26-489f-8b35-9b16c94a6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем DataFrame (это Transformation - ленивая операция)\n",
    "# Генератор данных: создаем список от 0 до 1 млн\n",
    "data = spark.range(0, 1000000)\n",
    "\n",
    "# Пока что Job не создан! Проверьте вкладку \"Jobs\" в UI - там пусто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45549ee6-dfda-4941-bed5-d5426f8bc4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Вызываем Action\n",
    "count = data.count()\n",
    "\n",
    "print(f\"Количество строк: {count}\")\n",
    "# ТЕПЕРЬ обновите вкладку \"Jobs\" в Spark UI. Появился Job Id 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd1dd3f-f0a6-4ad7-91e4-cead1c97e799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition вызывает Shuffle (перераспределение данных)\n",
    "# Это Transformation\n",
    "repartitioned_data = data.repartition(4)\n",
    "\n",
    "# Action - чтобы запустить выполнение\n",
    "repartitioned_data.count()\n",
    "\n",
    "# Идите в Spark UI -> Jobs.\n",
    "# Кликните на новый Job (Description).\n",
    "# Вы увидите DAG Visualization: там будет 2 квадрата (Stage), соединенных стрелкой Exchange.\n",
    "# Exchange = Shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0b0434-b9a5-4f59-8cc1-e199e8f43f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не забываем закрывать\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc56d52-d77b-4c25-b400-742dbda8e902",
   "metadata": {},
   "source": [
    "#### tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48209cb7-5a47-4e9a-895a-8b04a5cef4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Создаем сессию с явным указанием ресурсов для наглядности\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Module2_Architecture_Lab\")\n",
    "         .master(\"local[*]\") # Используем все ядра локально\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bced31-2198-4253-bb37-05083913f279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Генерируем 10 млн чисел и разбиваем на 4 раздела (Tasks)\n",
    "# Все разделы будут одинакового размера (по 2.5 млн)\n",
    "df_balanced = spark.range(0, 10000000).repartition(4)\n",
    "\n",
    "# Action запускает Job\n",
    "df_balanced.count()\n",
    "\n",
    "# ЗАДАНИЕ:\n",
    "# 1. Зайдите в Stages. Найдите последний Stage.\n",
    "# 2. Посмотрите Summary Metrics.\n",
    "# 3. Ожидание: Duration у Min и Max почти одинаковое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d256989-f44c-4c43-beb2-c21be6db3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Создаем данные, где 99% ключей = 1, а остальные случайные\n",
    "# Это классический случай \"товара-локомотива\" в продажах\n",
    "df_skewed = spark.range(0, 1000000).withColumn(\"key\", \n",
    "    F.when(F.col(\"id\") < 990000, 1).otherwise(F.col(\"id\"))\n",
    ")\n",
    "\n",
    "# Вызываем Shuffle (groupBy вызывает перемешивание данных)\n",
    "# Данные полетят на экзекьюторы в зависимости от ключа.\n",
    "# Все записи с ключом 1 (990 тыс. штук) полетят на ОДИН Task.\n",
    "df_skewed.groupBy(\"key\").count().collect()\n",
    "\n",
    "# ЗАДАНИЕ:\n",
    "# 1. Зайдите в Stages. Найдите Stage с \"Aggregate\".\n",
    "# 2. Посмотрите Summary Metrics.\n",
    "# 3. Ожидание: Max Duration будет в разы больше Median.\n",
    "# 4. В Event Timeline одна полоска будет очень длинной, остальные короткие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4046b-4b6b-44ba-a096-9ed4d17c775b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

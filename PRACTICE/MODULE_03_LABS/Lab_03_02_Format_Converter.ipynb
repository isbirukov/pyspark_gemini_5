{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e1ad8e-a581-42f8-a245-ad8ab70ccd16",
   "metadata": {},
   "source": [
    "### Задача 1: JSON Hell\n",
    "\n",
    "- Сгенерируйте данные и сохраните их в JSON.\n",
    "- Попробуйте прочитать JSON.\n",
    "- Теперь сохраните JSON с опцией .option(\"multiline\", \"true\") (представьте, что одна запись занимает несколько строк). Прочитайте обратно.\n",
    "\n",
    "Цель: Понять, что JSON — сложный формат, и опции решают всё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2adb84c-ced6-4b1d-aedd-d042aec61089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Work_with_JSON\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_data = [\n",
    "    (1, \"John\", 30, [\"Python\", \"Spark\"], {\"city\": \"Moscow\", \"salary\": 100000}),\n",
    "    (2, \"Alice\", 25, [\"Java\", \"SQL\"], {\"city\": \"SPb\", \"salary\": 90000}),\n",
    "    (3, \"Bob\", 35, [\"Scala\", \"Kafka\"], {\"city\": \"Moscow\", \"salary\": 120000})\n",
    "]\n",
    "\n",
    "data_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"salary\", IntegerType(), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab41ce4c-4379-4aed-bd25-b165246b9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df_data, schema=data_schema)\n",
    "\n",
    "path_dir = \"/home/jovyan/workspace/data/\"\n",
    "df.write.mode(\"overwrite\").json(path_dir + \"example.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49587a6-9d1b-4577-aed6-44d822ffb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---------------+----------------+\n",
      "| id| name|age|         skills|        metadata|\n",
      "+---+-----+---+---------------+----------------+\n",
      "|  1| John| 30|[Python, Spark]|{Moscow, 100000}|\n",
      "|  3|  Bob| 35| [Scala, Kafka]|{Moscow, 120000}|\n",
      "|  2|Alice| 25|    [Java, SQL]|    {SPb, 90000}|\n",
      "+---+-----+---+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# чтение json файла\n",
    "json_df = spark.read \\\n",
    "        .schema(data_schema) \\\n",
    "        .json(path_dir + \"example.json\")\n",
    "json_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc873d7d-961c-4d5e-bfb7-7bb93b2c00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл успешно сохранен!\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\") \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .json(path_dir + \"example.json\")\n",
    "print(\"Файл успешно сохранен!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d039416f-39e2-4017-9be3-3ce0c872eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---------------+----------------+\n",
      "| id| name|age|         skills|        metadata|\n",
      "+---+-----+---+---------------+----------------+\n",
      "|  1| John| 30|[Python, Spark]|{Moscow, 100000}|\n",
      "|  3|  Bob| 35| [Scala, Kafka]|{Moscow, 120000}|\n",
      "|  2|Alice| 25|    [Java, SQL]|    {SPb, 90000}|\n",
      "+---+-----+---+---------------+----------------+\n",
      "\n",
      "Многострочный JSON:\n",
      "+---+---+----------------+-----+---------------+\n",
      "|age| id|        metadata| name|         skills|\n",
      "+---+---+----------------+-----+---------------+\n",
      "| 30|  1|{Moscow, 100000}| John|[Python, Spark]|\n",
      "| 35|  3|{Moscow, 120000}|  Bob| [Scala, Kafka]|\n",
      "| 25|  2|    {SPb, 90000}|Alice|    [Java, SQL]|\n",
      "+---+---+----------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# чтение json файла\n",
    "json_df_1 = spark.read \\\n",
    "        .schema(data_schema) \\\n",
    "        .json(path_dir + \"example.json\")\n",
    "json_df_1.show()\n",
    "\n",
    "multiline_json = spark.read.option(\"multiline\", \"true\").json(path_dir + \"example.json\")\n",
    "print(\"Многострочный JSON:\")\n",
    "multiline_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4a96d-13df-4b37-a62b-c0066e9c8f50",
   "metadata": {},
   "source": [
    "### Задача 2: Partition Pruning (Оптимизация)\n",
    "\n",
    "1. Используйте processed_sales (Parquet из практики).\n",
    "2. Прочитайте его обратно: spark.read.parquet(...).\n",
    "3. Сделайте фильтр: df.filter(\"country = 'US'\").explain().\n",
    "4. В выводе explain найдите фразу PartitionFilters.\n",
    "\n",
    "Если она есть: Поздравляю! Spark понял, что читать папки CN и KR не нужно. Он пошел только в папку US. Это основа оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1cb8ce-fef2-4929-9c36-cfc23f6e9c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+-------+\n",
      "|      date|product|amount|country|\n",
      "+----------+-------+------+-------+\n",
      "|2023-01-01|Samsung|   900|     KR|\n",
      "|2023-01-02| Xiaomi|   400|     CN|\n",
      "|2023-01-01| iPhone|  1000|     US|\n",
      "|2023-01-02| iPhone|  1000|     US|\n",
      "+----------+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read \\\n",
    "        .format(\"parquet\") \\\n",
    "        .load(\"/home/jovyan/workspace/data/processed_sales\")\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5cb7a8-29d8-4ffd-af88-0f5ff2b83bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [date#186,product#187,amount#188,country#189] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/workspace/data/processed_sales], PartitionFilters: [isnotnull(country#189), (country#189 = US)], PushedFilters: [], ReadSchema: struct<date:string,product:string,amount:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.filter(\"country = 'US'\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5033c-9d8e-499c-810f-b7298cab93d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
